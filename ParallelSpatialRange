import java.awt.geom.Point2D;
import java.awt.geom.Rectangle2D;
import java.io.IOException;
import java.net.URI;
import java.util.Arrays;
import java.util.List;
import java.util.Scanner;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.apache.spark.api.java.*;
import org.apache.spark.api.java.function.*;
import org.apache.spark.SparkConf;

import scala.Tuple2;
import scala.Tuple4;


public class SpatialRange{
	
	public  static void main(String[] args){

		Scanner scan = new Scanner(System.in);
		System.out.println("This is spatial range query");
		System.out.println("Please enter the path of the source file in HDFS:");
		String sHadoop = scan.next();
		System.out.println("Please enter the query window of range query in HDFS:");
		String querywindow = scan.next();
		URI uri=URI.create("hdfs://172.31.9.118:54310/test/tempResult.txt");
		Path pathhadoop=new Path(uri);
		Configuration confhadoop=new Configuration();
		confhadoop.set("fs.hdfs.impl", 
			        org.apache.hadoop.hdfs.DistributedFileSystem.class.getName()
			    );
		confhadoop.set("fs.file.impl",
			        org.apache.hadoop.fs.LocalFileSystem.class.getName()
			    );
		try {
			FileSystem filehadoop =FileSystem.get(uri, confhadoop);
			filehadoop.delete(pathhadoop, true);
			System.out.println("Old output file has been deleted!");
			//;
		} catch (IOException e) {
			e.printStackTrace();
		}
		RangeQuery(sHadoop,"hdfs://172.31.9.118:54310/test/tempResult.txt",Double.parseDouble(Arrays.asList(querywindow.split(",")).get(0)),Double.parseDouble(Arrays.asList(querywindow.split(",")).get(1)),Double.parseDouble(Arrays.asList(querywindow.split(",")).get(2)),Double.parseDouble(Arrays.asList(querywindow.split(",")).get(3)));
		scan.close();
	}
	  public static void RangeQuery(String InputLocation, String OutputLocation, final double LongitudeQueryPoint1,final double LatitudeQueryPoint1,final double LongitudeQueryPoint2,final double LatitudeQueryPoint2)	
	{
		
		SparkConf conf=new SparkConf().setAppName("SpatialRangeQuery").setMaster("spark://172.31.9.118:7077").set("spark.local.ip", "172.31.9.118").set("spark.driver.host", "172.31.9.118").set("spark.eventLog.enabled","true").set("spark.eventLog.dir", "/tmp/spark-events/");
		JavaSparkContext spark=new JavaSparkContext(conf);
		spark.addJar("target/operation-0.0.1-SNAPSHOT-rangeedges.jar");
		
		//Input data from HDFS into RDD
		JavaRDD<String> file = spark.textFile(InputLocation);
		System.out.println("Data has been loaded!");
		//This JavaPairRDD structure: gid,0 or 1. This RDD contains all every row in the original dataset.
		JavaRDD<Tuple4<Double,Double,Double,Double>> calculatedRDD=file.map(new Function<String,Tuple4<Double,Double,Double,Double>>()
				{
					public Tuple4<Double,Double,Double,Double> call(String s)
					{
						
						//int gid=Integer.parseInt(Arrays.asList(s.split(",")).get(1));
						double x1;
						double y1;
						double x2;
						double y2;
						try{
						x1=Double.parseDouble(Arrays.asList(s.split(",")).get(2));
						y1=Double.parseDouble(Arrays.asList(s.split(",")).get(3));
						x2=Double.parseDouble(Arrays.asList(s.split(",")).get(4));
						y2=Double.parseDouble(Arrays.asList(s.split(",")).get(5));
						}
						catch(Exception e)
						{
						x1=0.00;
						y1=0.00;
						x2=0.00;
						y2=0.00;
						}
						//double xCenter=(x1+x2)*0.5;
						//double yCenter=(y1+y2)*0.5;
						double xDownerBoundary=0;
						double xUpperBoundary=0;
						double yDownerBoundary=0;
						double yUpperBoundary=0;
						/*Rectangle2D.Double queryRectangle=new Rectangle2D.Double(LongitudeQueryPoint,LatitudeQueryPoint,LengthHorizontal,LengthVertical);
						Rectangle2D.Double RDDRectangle=new Rectangle2D.Double(x1, y1,Math.abs(x1-x2),Math.abs(y1-y2));
						//Can judge if this rectangle contains one point
						Point2D.Double RDDPoint=new Point2D.Double((x1+x2)*0.5,(y1+y2)*0.5);
						if(queryRectangle.contains(RDDRectangle))
							{
							return new Tuple2(gid,1);
							}
						else
						{
							return new Tuple2(gid,0);
						}*/
						
						if(LongitudeQueryPoint1>LongitudeQueryPoint2)
						{
							xUpperBoundary=LongitudeQueryPoint1;
							xDownerBoundary=LongitudeQueryPoint2;
						}
						else 
						{
							xUpperBoundary=LongitudeQueryPoint2;
							xDownerBoundary=LongitudeQueryPoint1;
						}
						if(LatitudeQueryPoint1>LatitudeQueryPoint2)
						{
							yUpperBoundary=LatitudeQueryPoint1;
							yDownerBoundary=LatitudeQueryPoint2;
						}
						else
						{
							yUpperBoundary=LatitudeQueryPoint2;
							yDownerBoundary=LatitudeQueryPoint1;
						}
						if(x1>=xDownerBoundary && x1<=xUpperBoundary && x2>=xDownerBoundary&&x2<=xUpperBoundary)
						
						{
							if(y1>=yDownerBoundary && y1<=xUpperBoundary && y2>=yDownerBoundary&&y2<=yUpperBoundary)
							{return new Tuple4(x1,y1,x2,y2);}
							else return new Tuple4(0.0,0.0,0.0,0.0);
						
							
						}
						else return new Tuple4(0.0,0.0,0.0,0.0);
						
					}
				});
		
	
		//This JavaPairRDD structure: gid, 1. This RDD only contains the row whose second column is 1.
		JavaRDD<Tuple4<Double,Double,Double,Double>> preResultRDD=calculatedRDD.filter(new Function<Tuple4<Double,Double,Double,Double>,Boolean>()
				{
					public Boolean call(Tuple4<Double,Double,Double,Double> s)
					{
						if(s._1()==0.00&&s._2()==0.0&&s._3()==0.0&&s._4()==0.0)
						{
							return false;
						}
						else
						{
							return true;
						}
					}
				});
		/*

		//Output the resultRDD into HDFS
		System.out.println("Data has been processed!");
		preResultRDD.saveAsTextFile(OutputLocation);
		System.out.println("Data has been saved!");
		spark.stop();
		}
}
